{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def loadData(data_set = None):\n",
    "\n",
    "    if (data_set == None):\n",
    "        return None\n",
    "    else:\n",
    "        data = []\n",
    "        fptr = open(data_set, \"r\")\n",
    "        lines = fptr.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip().split()\n",
    "            if (line[-1] == -1):\n",
    "                continue\n",
    "            for i in range(len(line)):\n",
    "                line[i] = float(line[i])\n",
    "            # print(line)\n",
    "            data.append(line)\n",
    "        \n",
    "        return np.array(data)\n",
    "\n",
    "data_set = loadData(\"3_19/visual_annotion_3_19.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26976\n"
     ]
    }
   ],
   "source": [
    "print(len(data_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_set[:, :-1]\n",
    "y = data_set[:, -1]\n",
    "\n",
    "# print(X[:5], Y[:5], data_set[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "another_data_Set = np.array(loadData(\"5_11/visual_annotion_5_11.txt\"))\n",
    "\n",
    "X_new = another_data_Set[:, :-1]\n",
    "Y_new = another_data_Set[:, -1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility function that are used in every model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "def printMetrics(y_test, y_pred):\n",
    "    print(\"Accuracy: \", accuracy_score(y_pred, y_test)*100)\n",
    "    print(\"Mean Squared Error: \", mean_squared_error(y_pred, y_test))\n",
    "    print(\"Mean Absolute Error: \", mean_absolute_error(y_pred, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# splitting the data set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model-1 Descision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing libraries \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Model training\n",
    "model_1 = DecisionTreeClassifier()\n",
    "model_1.fit(X, y)\n",
    "\n",
    "# y_pred = model_1.predict(X_new)\n",
    "# printMetrics(Y_new, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model-2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  92.0867309117865\n",
      "Mean Squared Error:  0.0813565604151223\n",
      "Mean Absolute Error:  0.07987398072646405\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model_2 = RandomForestClassifier()\n",
    "model_2.fit(X, y)\n",
    "\n",
    "y_pred = model_2.predict(X_test)\n",
    "printMetrics(y_test, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model-3 K-Nearest Neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  77.79836916234247\n",
      "Mean Squared Error:  0.24147516679021497\n",
      "Mean Absolute Error:  0.22850259451445515\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model_3 = KNeighborsClassifier(n_neighbors=5)\n",
    "model_3.fit(X, y)\n",
    "\n",
    "y_pred = model_3.predict(X_test)\n",
    "printMetrics(y_test, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model-4 AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy\n",
      "Accuracy:  67.19184430027804\n",
      "Mean Squared Error:  0.3556070435588508\n",
      "Mean Absolute Error:  0.33725671918443\n",
      "\n",
      "test accuracy\n",
      "Accuracy:  66.27131208302445\n",
      "Mean Squared Error:  0.3617494440326168\n",
      "Mean Absolute Error:  0.34544106745737585\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "model_4 = AdaBoostClassifier()\n",
    "model_4.fit(X_train, y_train)\n",
    "\n",
    "print(\"training accuracy\")\n",
    "y_pred = model_4.predict(X_train)\n",
    "printMetrics(y_train, y_pred)\n",
    "\n",
    "print(\"\\ntest accuracy\")\n",
    "y_pred = model_4.predict(X_test)\n",
    "printMetrics(y_pred, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model-5 Naive Bayes (Gaussian Naive Bayes classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy\n",
      "Accuracy:  65.7321594068582\n",
      "Mean Squared Error:  0.37451343836886003\n",
      "Mean Absolute Error:  0.35329008341056534\n",
      "\n",
      "test accuracy\n",
      "Accuracy:  65.99332839140104\n",
      "Mean Squared Error:  0.36730911786508524\n",
      "Mean Absolute Error:  0.3491475166790215\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model_5 = GaussianNB()\n",
    "model_5.fit(X_train, y_train)\n",
    "\n",
    "print(\"training accuracy\")\n",
    "y_pred = model_5.predict(X_train)\n",
    "printMetrics(y_train, y_pred)\n",
    "\n",
    "print(\"\\ntest accuracy\")\n",
    "y_pred = model_5.predict(X_test)\n",
    "printMetrics(y_pred, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model-6 Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy\n",
      "Accuracy:  67.67840593141798\n",
      "Mean Squared Error:  0.34976830398517145\n",
      "Mean Absolute Error:  0.33206672845227064\n",
      "\n",
      "test accuracy\n",
      "Accuracy:  67.67976278724981\n",
      "Mean Squared Error:  0.34321719792438843\n",
      "Mean Absolute Error:  0.32987398072646407\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model_6 = SVC()\n",
    "model_6.fit(X_train, y_train)\n",
    "\n",
    "print(\"training accuracy\")\n",
    "y_pred = model_6.predict(X_train)\n",
    "printMetrics(y_train, y_pred)\n",
    "\n",
    "print(\"\\ntest accuracy\")\n",
    "y_pred = model_6.predict(X_test)\n",
    "printMetrics(y_pred, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model-7 Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy\n",
      "Accuracy:  70.9082483781279\n",
      "Mean Squared Error:  0.3156626506024096\n",
      "Mean Absolute Error:  0.2991658943466172\n",
      "\n",
      "test accuracy\n",
      "Accuracy:  70.23721275018532\n",
      "Mean Squared Error:  0.31597479614529284\n",
      "Mean Absolute Error:  0.3037435137138621\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "model_7 = GradientBoostingClassifier()\n",
    "model_7.fit(X_train, y_train)\n",
    "\n",
    "print(\"training accuracy\")\n",
    "y_pred = model_7.predict(X_train)\n",
    "printMetrics(y_train, y_pred)\n",
    "\n",
    "print(\"\\ntest accuracy\")\n",
    "y_pred = model_7.predict(X_test)\n",
    "printMetrics(y_pred, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model-8 Neural Networks (Multi-Layer Perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy\n",
      "Accuracy:  68.08155699721965\n",
      "Mean Squared Error:  0.3551899907321594\n",
      "Mean Absolute Error:  0.33118628359592217\n",
      "\n",
      "test accuracy\n",
      "Accuracy:  67.82802075611563\n",
      "Mean Squared Error:  0.3539659006671609\n",
      "Mean Absolute Error:  0.33246849518161603\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model_8 = MLPClassifier()\n",
    "model_8.fit(X_train, y_train)\n",
    "\n",
    "print(\"training accuracy\")\n",
    "y_pred = model_8.predict(X_train)\n",
    "printMetrics(y_train, y_pred)\n",
    "\n",
    "print(\"\\ntest accuracy\")\n",
    "y_pred = model_8.predict(X_test)\n",
    "printMetrics(y_pred, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual Class Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model                  True Accuracy    False Accuracy\n",
      "-------------------  ---------------  ----------------\n",
      "Descision tree               85.6512           96.7665\n",
      "Random Forest                89.038            93.9601\n",
      "K-Nearest Neighbour          74.681            80.9992\n",
      "AdaBoost                     52.6669           78.891\n",
      "Naive Bayes                  43.8563           83.9547\n",
      "SVM                          46.2287           85.4528\n",
      "Gradient Boosting            59.285            80.2942\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "def individualClassAccuracy(data_set):\n",
    "\n",
    "    X_true = []\n",
    "    y_true = []\n",
    "    X_false = []\n",
    "    y_false = []\n",
    "\n",
    "    X = data_set[:, :-1]\n",
    "    y = data_set[:, -1]\n",
    "\n",
    "    for i in range(len(data_set)):\n",
    "        \n",
    "        if data_set[i, -1] == 1:\n",
    "            X_true.append(data_set[i, :-1])\n",
    "            y_true.append(data_set[i, -1])\n",
    "        else:\n",
    "            X_false.append(data_set[i, :-1])\n",
    "            y_false.append(data_set[i, -1])\n",
    "\n",
    "    \n",
    "    outputs = [[\"Model\", \"True Accuracy\", \"False Accuracy\"]]\n",
    "\n",
    "    outputs.append([\"Descision tree\"])\n",
    "\n",
    "    # Model 1: Descision tree\n",
    "    # print(\"for True value\\n\")\n",
    "\n",
    "    y_pred = model_1.predict(X_true)\n",
    "    outputs[1].append(accuracy_score(y_true, y_pred) * 100)\n",
    "    y_pred = model_1.predict(X_false)\n",
    "    outputs[1].append(accuracy_score(y_pred, y_false) * 100)\n",
    "\n",
    "    # Model: 2 Random Forest\n",
    "    outputs.append([\"Random Forest\"])\n",
    "    y_pred = model_2.predict(X_true)\n",
    "    outputs[2].append(accuracy_score(y_true, y_pred) * 100)\n",
    "    y_pred = model_2.predict(X_false)\n",
    "    outputs[2].append(accuracy_score(y_pred, y_false) * 100)\n",
    "\n",
    "    # Model: 3 K-Nearest Neighbour\n",
    "    outputs.append([\"K-Nearest Neighbour\"])\n",
    "    y_pred = model_3.predict(X_true)\n",
    "    outputs[3].append(accuracy_score(y_true, y_pred) * 100)\n",
    "    y_pred = model_3.predict(X_false)\n",
    "    outputs[3].append(accuracy_score(y_pred, y_false) * 100)\n",
    "\n",
    "    # Model: 4 AdaBoost\n",
    "    outputs.append([\"AdaBoost\"])\n",
    "    y_pred = model_4.predict(X_true)\n",
    "    outputs[4].append(accuracy_score(y_true, y_pred) * 100)\n",
    "    y_pred = model_4.predict(X_false)\n",
    "    outputs[4].append(accuracy_score(y_pred, y_false) * 100)\n",
    "\n",
    "    # Model: 5 Naive Bayes\n",
    "    outputs.append([\"Naive Bayes\"])\n",
    "    y_pred = model_5.predict(X_true)\n",
    "    outputs[5].append(accuracy_score(y_true, y_pred) * 100)\n",
    "    y_pred = model_5.predict(X_false)\n",
    "    outputs[5].append(accuracy_score(y_pred, y_false) * 100)\n",
    "\n",
    "    # Model: 6 SVM\n",
    "    outputs.append([\"SVM\"])\n",
    "    y_pred = model_6.predict(X_true)\n",
    "    outputs[6].append(accuracy_score(y_true, y_pred) * 100)\n",
    "    y_pred = model_6.predict(X_false)\n",
    "    outputs[6].append(accuracy_score(y_pred, y_false) * 100)\n",
    "\n",
    "    # Model: 7 Gradient Boosting\n",
    "    outputs.append([\"Gradient Boosting\"])\n",
    "    y_pred = model_7.predict(X_true)\n",
    "    outputs[7].append(accuracy_score(y_true, y_pred) * 100)\n",
    "    y_pred = model_7.predict(X_false)\n",
    "    outputs[7].append(accuracy_score(y_pred, y_false) * 100)\n",
    "    \n",
    "\n",
    "    print(tabulate(outputs, headers=\"firstrow\"))\n",
    "\n",
    "individualClassAccuracy(data_set)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find the breakpoint between true and false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 26\n"
     ]
    }
   ],
   "source": [
    "t_to_f = []\n",
    "f_to_t = []\n",
    "\n",
    "for i in range(len(data_set) - 1):\n",
    "\n",
    "    difference = lambda x, y : [a - b for a, b in zip(x, y)]\n",
    "    if data_set[i][-1] == 0 and data_set[i + 1][-1] == 1:\n",
    "        temp = difference(data_set[i], data_set[i + 1])\n",
    "        f_to_t.append(temp)\n",
    "    elif data_set[i][-1] == 1 and data_set[i + 1][-1] == 0:\n",
    "        temp = difference(data_set[i], data_set[i + 1])\n",
    "        t_to_f.append(temp)\n",
    "\n",
    "print(len(t_to_f), len(f_to_t))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run models at different size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def runModelAtSize(split_ratio, model_name, X, y):\n",
    "    \n",
    "    output = [round(split_ratio, 1), model_name]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=round(split_ratio, 1), random_state=42)\n",
    "\n",
    "    if model_name == \"Descison tree\":\n",
    "        model = DecisionTreeClassifier()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Train Accuracy\n",
    "        y_pred = model.predict(X_train)\n",
    "        output.append(accuracy_score(y_pred, y_train)*100)\n",
    "\n",
    "        # Test Accuracy\n",
    "        y_pred = model.predict(X_test)\n",
    "        output.append(accuracy_score(y_pred, y_test)*100)\n",
    "    \n",
    "    elif model_name == \"Random Forest\":\n",
    "        model = RandomForestClassifier()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Train Accuracy\n",
    "        y_pred = model.predict(X_train)\n",
    "        output.append(accuracy_score(y_pred, y_train)*100)\n",
    "\n",
    "        # Test Accuracy\n",
    "        y_pred = model.predict(X_test)\n",
    "        output.append(accuracy_score(y_pred, y_test)*100)\n",
    "    \n",
    "    elif model_name == \"KNN\":\n",
    "        model = KNeighborsClassifier()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Train Accuracy\n",
    "        y_pred = model.predict(X_train)\n",
    "        output.append(accuracy_score(y_pred, y_train)*100)\n",
    "\n",
    "        # Test Accuracy\n",
    "        y_pred = model.predict(X_test)\n",
    "        output.append(accuracy_score(y_pred, y_test)*100)\n",
    "\n",
    "    elif model_name == \"AdaBoost\":\n",
    "        model = AdaBoostClassifier()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Train Accuracy\n",
    "        y_pred = model.predict(X_train)\n",
    "        output.append(accuracy_score(y_pred, y_train)*100)\n",
    "\n",
    "        # Test Accuracy\n",
    "        y_pred = model.predict(X_test)\n",
    "        output.append(accuracy_score(y_pred, y_test)*100)\n",
    "    \n",
    "    elif model_name == \"Gaussian NB\":\n",
    "        model = GaussianNB()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Train Accuracy\n",
    "        y_pred = model.predict(X_train)\n",
    "        output.append(accuracy_score(y_pred, y_train)*100)\n",
    "\n",
    "        # Test Accuracy\n",
    "        y_pred = model.predict(X_test)\n",
    "        output.append(accuracy_score(y_pred, y_test)*100)\n",
    "    \n",
    "    elif model_name == \"SVM\":\n",
    "        model = SVC()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Train Accuracy\n",
    "        y_pred = model.predict(X_train)\n",
    "        output.append(accuracy_score(y_pred, y_train)*100)\n",
    "\n",
    "        # Test Accuracy\n",
    "        y_pred = model.predict(X_test)\n",
    "        output.append(accuracy_score(y_pred, y_test)*100)\n",
    "    \n",
    "    elif model_name == \"Gradient Boosting\":\n",
    "        model = GradientBoostingClassifier()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Train Accuracy\n",
    "        y_pred = model.predict(X_train)\n",
    "        output.append(accuracy_score(y_pred, y_train)*100)\n",
    "\n",
    "        # Test Accuracy\n",
    "        y_pred = model.predict(X_test)\n",
    "        output.append(accuracy_score(y_pred, y_test)*100)\n",
    "    \n",
    "    elif model_name == \"MLP\":\n",
    "        model = MLPClassifier()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Train Accuracy\n",
    "        y_pred = model.predict(X_train)\n",
    "        output.append(accuracy_score(y_pred, y_train)*100)\n",
    "\n",
    "        # Test Accuracy\n",
    "        y_pred = model.predict(X_test)\n",
    "        output.append(accuracy_score(y_pred, y_test)*100)\n",
    "    \n",
    "    \n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility function that run all the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split ratio    Model              Train Accuracy     Test Accuracy\n",
      "-------------  -----------------  -----------------  -----------------\n",
      "0.2            Descison tree      92.451343836886    64.93699036323203\n",
      "0.2            Random Forest      92.451343836886    68.51371386212008\n",
      "0.2            KNN                78.18350324374421  68.55077835433654\n",
      "0.2            AdaBoost           67.19184430027804  66.27131208302445\n",
      "0.2            Gaussian NB        65.7321594068582   65.99332839140104\n",
      "0.2            SVM                67.67840593141798  67.67976278724981\n",
      "0.2            Gradient Boosting  70.9082483781279   70.23721275018532\n",
      "0.2            MLP                66.12604263206673  65.82653817642698\n",
      "-------------  -------------      -------------      -------------\n",
      "0.3            Descison tree      93.06783879680137  64.94501420981095\n",
      "0.3            Random Forest      93.06783879680137  68.44186333868775\n",
      "0.3            KNN                78.29264417730234  68.60249598418386\n",
      "0.3            AdaBoost           67.16623417889107  66.94674409983938\n",
      "0.3            Gaussian NB        65.64634856749457  65.97059186951687\n",
      "0.3            SVM                67.5475295239104   67.84875818608674\n",
      "0.3            Gradient Boosting  70.95270878568024  70.3570987272952\n",
      "0.3            MLP                67.77524757718582  67.7499073273199\n",
      "-------------  -------------      -------------      -------------\n",
      "0.4            Descison tree      93.58665430954588  64.14604763228617\n",
      "0.4            Random Forest      93.58665430954588  68.21425261792234\n",
      "0.4            KNN                78.25764596848934  68.15865072745807\n",
      "0.4            AdaBoost           68.10009267840593  67.73236956723196\n",
      "0.4            Gaussian NB        65.63484708063021  65.81410434621444\n",
      "0.4            SVM                67.71084337349397  67.40802520619035\n",
      "0.4            Gradient Boosting  71.19555143651529  70.09544991196367\n",
      "0.4            MLP                67.17948717948717  66.5739968492262\n",
      "-------------  -------------      -------------      -------------\n",
      "0.5            Descison tree      94.2541518386714   64.35349940688019\n",
      "0.5            Random Forest      94.2541518386714   68.17912218268091\n",
      "0.5            KNN                78.1731909845789   68.31998813760379\n",
      "0.5            AdaBoost           67.46737841043891  67.28944246737841\n",
      "0.5            Gaussian NB        65.54715302491103  66.073546856465\n",
      "0.5            SVM                67.60083036773428  67.63790035587188\n",
      "0.5            Gradient Boosting  71.16696322657177  70.26245551601423\n",
      "0.5            MLP                68.29033214709371  67.6749703440095\n",
      "-------------  -------------      -------------      -------------\n",
      "0.6            Descison tree      94.98609823911028  63.97504015816138\n",
      "0.6            Random Forest      94.97683039851714  67.97232175954528\n",
      "0.6            KNN                78.00741427247452  68.97936488323242\n",
      "0.6            AdaBoost           66.39481000926783  66.28567898183616\n",
      "0.6            Gaussian NB        65.48656163113994  66.04473001359199\n",
      "0.6            SVM                67.32159406858203  67.61398739651551\n",
      "0.6            Gradient Boosting  71.67747914735867  70.30767329791178\n",
      "0.6            MLP                68.07228915662651  67.53367107376745\n",
      "-------------  -------------      -------------      -------------\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "def runModels(data_set = None):\n",
    "    X = data_set[:, :-1]\n",
    "    y = data_set[:, -1]\n",
    "\n",
    "    \n",
    "    outputs = [[\"Split ratio\", \"Model\", \"Train Accuracy\", \"Test Accuracy\"]]\n",
    "    \n",
    "    # Run model 1: Descision tree\n",
    "    split_ratio = 0.2\n",
    "    for i in range(5):\n",
    "        outputs.append(runModelAtSize(split_ratio, \"Descison tree\", X, y))\n",
    "        outputs.append(runModelAtSize(split_ratio, \"Random Forest\", X, y))\n",
    "        outputs.append(runModelAtSize(split_ratio, \"KNN\", X, y))\n",
    "        outputs.append(runModelAtSize(split_ratio, \"AdaBoost\", X, y))\n",
    "        outputs.append(runModelAtSize(split_ratio, \"Gaussian NB\", X, y))\n",
    "        outputs.append(runModelAtSize(split_ratio, \"SVM\", X, y))\n",
    "        outputs.append(runModelAtSize(split_ratio, \"Gradient Boosting\", X, y))\n",
    "        outputs.append(runModelAtSize(split_ratio, \"MLP\", X, y))\n",
    "\n",
    "\n",
    "        outputs.append([\"-------------\"] * len(outputs[0]))\n",
    "        split_ratio += 0.1\n",
    "\n",
    "    print(tabulate(outputs, headers=\"firstrow\"))\n",
    "\n",
    "runModels(data_set)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combined Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined accuracy\n",
      "Model                Train Accuracy    Test Accuracy\n",
      "-----------------  ----------------  ---------------\n",
      "Descison tree               92.4513          64.937\n",
      "Random Forest               92.4513          68.662\n",
      "KNN                         78.1835          68.5508\n",
      "AdaBoost                    67.1918          66.2713\n",
      "Gaussian NB                 65.7322          65.9933\n",
      "SVM                         67.6784          67.6798\n",
      "Gradient Boosting           70.9082          70.2372\n",
      "MLP                         68.202           68.1616\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def runModelCombine(model_name, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    output = [model_name]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    if model_name == \"Descison tree\":\n",
    "        model = DecisionTreeClassifier()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Train Accuracy\n",
    "        y_pred = model.predict(X_train)\n",
    "        output.append(accuracy_score(y_pred, y_train)*100)\n",
    "\n",
    "        # Test Accuracy\n",
    "        y_pred = model.predict(X_test)\n",
    "        output.append(accuracy_score(y_pred, y_test)*100)\n",
    "    \n",
    "    elif model_name == \"Random Forest\":\n",
    "        model = RandomForestClassifier()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Train Accuracy\n",
    "        y_pred = model.predict(X_train)\n",
    "        output.append(accuracy_score(y_pred, y_train)*100)\n",
    "\n",
    "        # Test Accuracy\n",
    "        y_pred = model.predict(X_test)\n",
    "        output.append(accuracy_score(y_pred, y_test)*100)\n",
    "    \n",
    "    elif model_name == \"KNN\":\n",
    "        model = KNeighborsClassifier()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Train Accuracy\n",
    "        y_pred = model.predict(X_train)\n",
    "        output.append(accuracy_score(y_pred, y_train)*100)\n",
    "\n",
    "        # Test Accuracy\n",
    "        y_pred = model.predict(X_test)\n",
    "        output.append(accuracy_score(y_pred, y_test)*100)\n",
    "    \n",
    "    elif model_name == \"AdaBoost\":\n",
    "        model = AdaBoostClassifier()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Train Accuracy\n",
    "        y_pred = model.predict(X_train)\n",
    "        output.append(accuracy_score(y_pred, y_train)*100)\n",
    "\n",
    "        # Test Accuracy\n",
    "        y_pred = model.predict(X_test)\n",
    "        output.append(accuracy_score(y_pred, y_test)*100)\n",
    "    \n",
    "    elif model_name == \"Gaussian NB\":\n",
    "        model = GaussianNB()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Train Accuracy\n",
    "        y_pred = model.predict(X_train)\n",
    "        output.append(accuracy_score(y_pred, y_train)*100)\n",
    "\n",
    "        # Test Accuracy\n",
    "        y_pred = model.predict(X_test)\n",
    "        output.append(accuracy_score(y_pred, y_test)*100)\n",
    "    \n",
    "    elif model_name == \"SVM\":\n",
    "        model = SVC()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Train Accuracy\n",
    "        y_pred = model.predict(X_train)\n",
    "        output.append(accuracy_score(y_pred, y_train)*100)\n",
    "\n",
    "        # Test Accuracy\n",
    "        y_pred = model.predict(X_test)\n",
    "        output.append(accuracy_score(y_pred, y_test)*100)\n",
    "    \n",
    "    elif model_name == \"Gradient Boosting\":\n",
    "        model = GradientBoostingClassifier()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Train Accuracy\n",
    "        y_pred = model.predict(X_train)\n",
    "        output.append(accuracy_score(y_pred, y_train)*100)\n",
    "\n",
    "        # Test Accuracy\n",
    "        y_pred = model.predict(X_test)\n",
    "        output.append(accuracy_score(y_pred, y_test)*100)\n",
    "    \n",
    "    elif model_name == \"MLP\":\n",
    "        model = MLPClassifier()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Train Accuracy\n",
    "        y_pred = model.predict(X_train)\n",
    "        output.append(accuracy_score(y_pred, y_train)*100)\n",
    "\n",
    "        # Test Accuracy\n",
    "        y_pred = model.predict(X_test)\n",
    "        output.append(accuracy_score(y_pred, y_test)*100)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def combineAccuracy(train_data, test_data):\n",
    "    X_train = train_data[:, :-1]\n",
    "    y_train = train_data[:, -1]\n",
    "\n",
    "    X_test = test_data[:, :-1]\n",
    "    y_test = test_data[:, -1]\n",
    "\n",
    "    print(\"combined accuracy\")\n",
    "    outputs = [[\"Model\", \"Train Accuracy\", \"Test Accuracy\"]]\n",
    "    \n",
    "    outputs.append(runModelCombine(\"Descison tree\", X_train, y_train, X_test, y_test))\n",
    "    outputs.append(runModelCombine(\"Random Forest\", X_train, y_train, X_test, y_test))\n",
    "    outputs.append(runModelCombine(\"KNN\", X_train, y_train, X_test, y_test))\n",
    "    outputs.append(runModelCombine(\"AdaBoost\", X_train, y_train, X_test, y_test))\n",
    "    outputs.append(runModelCombine(\"Gaussian NB\", X_train, y_train, X_test, y_test))\n",
    "    outputs.append(runModelCombine(\"SVM\", X_train, y_train, X_test, y_test))\n",
    "    outputs.append(runModelCombine(\"Gradient Boosting\", X_train, y_train, X_test, y_test))\n",
    "    outputs.append(runModelCombine(\"MLP\", X_train, y_train, X_test, y_test))\n",
    "\n",
    "    print(tabulate(outputs, headers=\"firstrow\"))\n",
    "\n",
    "train_data = loadData(\"3_19/visual_annotion_3_19.txt\")\n",
    "test_data = loadData(\"5_11/visual_annotion_5_11.txt\")\n",
    "\n",
    "combineAccuracy(train_data, test_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
